# 1️⃣ CNI

**💡CNI란?**

- `CNCF`에서 지정한 컨테이너용 '네트워크 플러그인 표준 스펙'
    - `컨테이너 런타임`과 `네트워크 프로비저닝` 플러그인을 분리하여 모듈화
- 파드 마다 `고유 IP를 부여하고 회수`하는 등의 관리 역할 및 `Pod CIDR 경로 설정` 등의 주요 기능 담당
    - 네트워크 네임스페이스(netns), 가상 이더넷 페어(veth), 커널 라우팅 테이블을 활용하여 위 기능 수행
    
![](https://velog.velcdn.com/images/alstjr971/post/67da5422-b927-48d8-b712-8966e2076901/image.png)

    

> 💡 네트워크 관련 요청이 들어오면 CNI가 이를 받아서 처리한다.

# 2️⃣ CNI 동작 방식

1. 컨테이너 생성 요청
    1. Kubelet이 `CRI`(Container Runtime Interfac) 에게 ‘컨테이너 생성’ 요청을 보낸다.
    2. CRI는 컨테이너를 띄워 준비 완료 신호를 반환한다.
2. 네트워크 준비 호출
    1. CRI는 ‘네트워크 설정’ 요청을
    2. CNI 플러그인에 보내 CNI 플러그인을 실행한다.

![](https://velog.velcdn.com/images/alstjr971/post/0cf2d789-8a78-43c4-820e-bcd6d533015d/image.png)


- 가상 인터페이스 생성 및 IP 할당
    1. CNI는 컨테이너 측과 호스트 측을 연결하는 veth 페어 생성
    2. IPAM 모듈을 통해 빈 IP 풀에서 주소를 파드에 할당
        - `IPAM` 모듈 : 주어진 (노드에 할당된) CIDR 중 하나를 할당
    3. 컨테이너 네임스페이스 안에서 인터페이스를 활성화하며 라우팅과 DNS 설정 적용
- 완료 응답 및 파드 실행
    1. CNI가 네트워크 구성 완료 신호를 kubelet에 전달
    2. 파드가 네트워크에 연결된 상태로 정상 실행하게 됨

>💡 정리하면, kubectl로 컨테이너 생성 요청을 하면 controller가 CRI 에게 명령을 전달하고, 컨테이너 생성을 한다.
- 이 상태까지 컨테이너는 인터페이스 없이 뜬다.
  - 마치 도커에서 network=none 으로 설정한 것과 유사
- 컨테이너 생성이 완료되면 해당 컨테이너에 연결할 네트워크 설정을 세팅하라고 CNI를 호출
  - 이때 가상 veth 인터페이스가 생성, 활성화되고 연결된다.

컨테이너 생성과 네트워크 설정을 ‘요청’ 한다고 하니까 마치 API 호출로 이뤄질 것 같지만, 실제 내부에서는 단지 명령어를 ‘실행’하는 로직으로 구성되어 있다.

(처음 CNI 사용 방법은 shell script 였다. 그 정도로 단순 작업이었음)

---

# 3️⃣ 주요 CNI 플러그인

## 1. Flannel
![](https://velog.velcdn.com/images/alstjr971/post/72e3d094-0089-4279-a61c-a27296c9bf19/image.png)

**쿠버네티스의 `모든 Pod가 하나의 VXLAN 안에서 통신`할 수 있게 해주는 CNI 플러그인**

- L2 기반 포워딩
- 노드별로 작은 서브넷을 할당해주고, 오버레이 네트워크로 묶어 구현
- 별도 기능 지원 없이 `최소한의 네트워킹만`으로 쿠버네티스 간 파드 연결성 지원

>💡 Flannel 플러그인은 어떤 추가 기능 없이 단순 네트워킹만 제공한다.
<br>
- overlay 네트워크 구축 기능만 존재
- 파드 간 연결 지원
- 주로 개발자, 테스터가 가볍게 사용할 때 사용




### 주요 특징 - Flannel

1. 간단한 설정
    - 최소한의 설정으로 빠른 구축 가능
    - 복잡한 네트워크 구성 없이 빠르게 네트워크 오버레이 구축 가능
2. 다양한 백엔드 지원
    - 네트워크 환경에 따라 백엔드 타입을 바꿔 `유연`하게 적용 가능
3. 쿠버네티스와의 높은 호환성
    - 쿠버네티스 네트워크 모델의 `표준에 맞춰 설계`되었음
    - 파드간 NAT 없는 통신, 노드 - Pod 통신, Host - Network 통신, IP 대역 중복 방지 등
4. 가벼운 리소스 사용
    - 데몬 하나(flannelId)만 실행됨
    - 복잡한 컨트롤 플레인이나 별도 데이터베이스 없이 etcd 등 간단한 key:value 저장소만 있으면 됨
    3. `CNI 중 가장 가벼운` 시스템 리소스 소모량을 가짐

**🔥 서브넷 할당** 
- Flannel 에이전트는 etcd/kubernetes API 에서 전체 파드 네트워크를 읽음
- 이후 각 노드에 고유한 서브넷 할당

**🔥 가상 인터페이스 구성**
- 각 노드는 할당된 서브넷의 식별자를 flannel 인터페이스에 설정
- 파드 브릿지에는 실제 파드 IP를 연결

![](https://velog.velcdn.com/images/alstjr971/post/4b7a64b7-fb9a-47f7-a5a0-582bf38302ae/image.png)


**🔥 호스트 라우팅 설정**
- 노드 OS의 라우팅 테이블에 Flannel 인터페이스 경로를 추가
- 로컬에서 원격 노드 서브넷으로 전달되는 트래픽 경로 지정

**🔥 캡슐화, 전달, 디캡슐화**
- 파드가 원격 노드로 패킷을 보내면 Flannel 인터페이스에서 캡슐화 후 네트워크를 통해 전송
- 수신 측 Flannel 인터페이스가 디캡슐화 진행 후 파드의 eth 로 전달

>💡 Flannel은 모든 정보를 읽거나 사용할 때 쿠버네티스 API를 이용한다.
<br>라우팅 테이블도 노드에 있는 라우팅 테이블을 그대로 사용한다.
<br>별 다른 작업 없이 리눅서 커널에서 제공되는 기능을 그대로 사용함을 의미
<br>iptables 에 규칙 몇 개 추가해서 사용

캡슐화/디캡슐화 → "터널 뚫어서 통신하게 해주겠다"

## 2. Calico

**`IP 라우팅`을 통해 서비스 간의 네트워크 트래픽을 제어하는 CNI 플러그인** 
- `L3 기반` 네트워크 모델(IP - in - IP)
- `IP 주소를 기반`으로 통신 수행

`Flannel`은 너무 가볍다 보니까 운영 환경에서 사용하는 데 한계가 있다.

![](https://velog.velcdn.com/images/alstjr971/post/4398cd27-653e-4a9b-a88a-46b5be6420b8/image.png)


`Calico` : 가상 머신 네트워크 설정하던 역사를 가진 회사

**flannel은 L2 기반 포워딩**

**Calico는 L3 기반 모델 사용 → IP 기반 라우팅
**
- L3 기반이면 라우팅 테이블에 존재하지 않는 경우 통신 가능한 다른 방법 X
    - → L3 를 사용하면 보안에 유리해서 선호됨.
- L2 기반에서 MAC 주소를 안다면, 우회해서 갈 수도 있음

기존 IP 패킷 앞에 `노드 IP 헤더`만 붙여서 보냄 (새로 막 캡슐화해서 만들고 보내지 않고)

### 주요 특징 - Calico

1. 뛰어난 성능
    - L3 기반 네트워크 모델을 사용해 오버레이 없이 직접 라우팅 수행
        - 낮은 지연 시간과 높은 네트워크 대역폭 제공
    - 파드 간, 파드 - 서비스 간 통신에서 가장 높은 네트워크 성능을 보임
2. 강력한 보안
    - 쿠버네티스 네트워크 정책을 확장해 파드, namespace, ip 대역 등 다양한 기준으로 트래픽 제어 가능
    - 보안 정책 변경 이력 관리, 시각화 등 고급 기능 제공
3. 확장성
    - 수천 개 노드와 파드를 처리할 수 있는 확장성을 갖춤
        - 대규모 환경에서도 네트워크 성능 저하 없이 운영
4. `BGP`(Border Gateway Protocol) 지원
    - 각 노드의 네트워크 정보를 동적으로 교환
    - `외부 네트워크와의 연동` 및 멀티 클러스터 환경에서 효율적인 라우팅 수행

**🔥 데이터 동기화**
- IP 풀, 워크로드, 네트워크 정책 정보를 Key:Value Store에 저장
    - Calicoctl 또는 K8S API 를 통해 전달

**🔥 Felix 에이전트**
- 각 노드에서 Felix 내부의 오케스트레이션 component가 컨테이너 생성과 삭제를 감지
- 라우팅 component가 BGP나 정책 기반 경로 수집

**🔥 커널 프로그래밍**
- Felix가 Linux 커널에 네트워크 정책을 구현
- 라우팅 테이블에 파드 IP 간 경로를 직접 설정하여 L3 네트워크 구현

**🔥 Legacy 호스트 지원**
- 물리 네트워크에 연결된 Legacay 호스트에도 Felix를 설치해 오버레이 없이 동일한 정책과 라우팅을 적용

![](https://velog.velcdn.com/images/alstjr971/post/f960bf3a-27a1-49e8-97ea-9728e401ce9f/image.png)


## 3. Cilium

**🔥 `eBPF`를 활용한 CNI 플러그인 (Extended Berkeley Packet Filter)**
- L3-`L7` 네트워크 정책
    - Label 및 Identity 기반 선언적 보안 제어 제공

![](https://velog.velcdn.com/images/alstjr971/post/41510427-01c7-436e-a4f9-676eb1128220/image.png)


- 최근 제일 각광 받고 있는 CNI 플러그인

기존에 Calico 의 장점 + 여러 솔루션들을 같이 사용 가능 (리눅스 feature, iptables .. 등)

### 주요 특징 - Cilium

1. **뛰어난 성능**
    - 낮은 지연시간과 높은 처리량 보장
        - eBPF 기술 사용
        - eBPF : 리눅스 커널에 안전하게 커스텀 코드를 삽입하고 실행할 수 있게 해주는 기술
        - 네트워크 패킷 처리, 추적, 보안 정책 등 에 사용
        - 커널 모듈을 만들지 않고도 강력한 성능과 유연성을 제공
2. **Pod의 인바운드로 들어오는 패킷을 캐치 후, 타겟 파드의 아웃 바운드로 바로 전달**
3. **이전 포스팅에서 설명한 기존의 전통적인 CNI 동작 방식(iptables)과 다른 방식**
   - pod → veth → veth → pod 방식으로 성능을 최적화
   - But, 패킷이 안보이거나 하는 문제점 존재
2. **네트워크 정책의 넓은 적용 범위**
    - 특정 파드, 서비스, 혹은 엔드포인트 간의 통신을 허용 및 제한 가능
        - 보안 및 네트워크 접근 제어 강화에 중요
    - L7에 대한 트래픽을 검사하고 제어할 수 있는 자체 API 기반 보안 기능 제공
3. **대규모 환경에서의 워크로드 통신 관리**
    - `CiliumMesh` 기능을 통해 여러 클러스터 간 네트워크 통신을 제어, 관리 가능

**🔥 cilium-agent → eBPF**
- Cilium-agent가 쿠버네티스 API에서 파드, 서비스, 정책 정보를 가져옴
- BPF 프로그램과 BPF 맵을 생성 후 커널에 로드

**🔥XDP/TC 훅에서 패킷을 가로챔**
- veth → host → veth 경로의 각 지점(XDP or TC)에 eBPF 프로그램을 붙임
    - 유저 스페이스로의 전환 없이 패킷 처리
- `XDP/TC` : iptable 에 오기 전에 패킷을 처리
    
![](https://velog.velcdn.com/images/alstjr971/post/475282ca-3088-4a2c-9d1f-27b4bb5c2fee/image.png)


- 왼쪽 그림 : 실제 패킷이 흘러가는 커널 구조(standard container networking)
- 오른쪽 그림 : `eBPF`
    - 오버헤드를 최소화
    - 패킷이 들어오자마자 catch 한 후, virtual interface에 바로 쏴버림

**🔥in-kernel L3/L4 포워딩 & 정책 적용**

- 커널 단에서 직접 필터링, 라우팅, 로드밸런싱을 수행

**🔥Iptables 오버헤드 제거 & 가시성**

- 기존 iptables 체인을 완전히 배제함으로써 컨텍스트 스위칭 비용을 최소화

>
- `calico`는 '네트워크 레벨'에서 최적화
  - calico는 IP 라우팅을 하고 (BGP 를 쓰니까)
- `cilium`은 '시스템 레벨'에서 최적화
  - cilium은 IP 라우팅 보단, IP 포워딩을 한다고 이해하면 된다.





